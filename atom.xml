<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhaoyupeng_blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-06T13:40:14.011Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>zhaoyupeng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy-入门</title>
    <link href="http://example.com/2022/05/06/Scrapy-%E5%85%A5%E9%97%A8/"/>
    <id>http://example.com/2022/05/06/Scrapy-%E5%85%A5%E9%97%A8/</id>
    <published>2022-05-06T13:40:09.000Z</published>
    <updated>2022-05-06T13:40:14.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h1><h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><p>完成下列任务:</p><ul><li><p>创建一个Scrapy项目</p></li><li><p>定义提取的结构化数据(Item)</p></li><li><p>编写爬取网站的 spider 并提取出结构化数据(Item)</p></li><li><p>编写 Item Pipeline 来存储提取到的Item(即结构化数据)</p></li><li></li></ul><h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p>在开始爬取之前，您必须创建一个新的<code>Scrapy</code>项目。 进入您打算存储代码的目录中，运行下列命令:</p><p> 复制代码</p><ol><li><code>scrapy startproject tutorial</code></li></ol><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/b0939eadc6fdbb910806f8ccb68db4ad.gif" alt="入门教程 - 图1"></p><p>该命令将会创建包含下列内容的 tutorial 目录:</p><p>这些文件分别是:</p><p> 复制代码</p><ol><li><code>scrapy.cfg: 项目的配置文件；（用于发布到服务器）</code></li><li><code>tutorial/: 该项目文件夹。之后将在此编写Python代码。</code></li><li><code>tutorial/items.py: 项目中的item文件;（定义结构化数据字段field）.</code></li><li><code>tutorial/pipelines.py: 项目中的pipelines文件;（用于存放执行后期数据处理的功能，定义如何存储结构化数据)</code></li><li><code>tutorial/settings.py: 项目的设置文件；(如何修改User-Agent，设置爬取时间间隔，设置代理，配置中间件等等)</code></li><li><code>tutorial/spiders/: 放置spider代码的目录;（编写爬取网站规则）</code></li></ol><h3 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h3><p>Item 定义结构化数据字段，用来保存爬取到的数据；其使用方法和python字典类似</p><p>可以通过创建一个 <code>scrapy.Item</code> 类， 并且定义类型为 <code>scrapy.Field</code>的类属性来定义一个Item。</p><p>首先根据需要从<a href="http://hr.tencent.com/position.php?&start=0#a">腾讯招聘</a>获取到的数据对item进行建模。 我们需要从<code>腾讯招聘</code>中获取 职位名称、<code>职位详情页url</code>、职位类别、人数、工作地点以及发布时间。 对此，在item中定义相应的字段。编辑 <code>tutorial</code> 目录中的 <code>items.py</code> 文件:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitItem(scrapy.Item):</code></p></li><li><p><code>name = scrapy.Field()</code></p></li><li><p><code>detailLink = scrapy.Field()</code></p></li><li><p><code>catalog = scrapy.Field()</code></p></li><li><p><code>recruitNumber = scrapy.Field()</code></p></li><li><p><code>workLocation = scrapy.Field()</code></p></li><li><p><code>publishTime = scrapy.Field()</code></p></li></ol><h3 id="编写第一个爬虫-Spider"><a href="#编写第一个爬虫-Spider" class="headerlink" title="编写第一个爬虫(Spider)"></a>编写第一个爬虫(Spider)</h3><p>Spider是开发者编写用于从单个网站(或者一些网站)爬取数据的类。</p><p>创建一个Spider，必须继承 ‘scrapy.Spider’ 类， 需要定义以下三个属性:</p><ul><li><p>name:</p><p>  spider名字；必须是唯一的</p></li><li><p>start_urls:</p><p>  初始的URL列表</p></li><li><p>parse(self, response)：</p><p>  每个初始URL完成下载后被调用</p><p>  这个函数要完成的功能：</p></li></ul><p> 复制代码</p><ol><li><code>1.负责解析返回的网页数据(response.body)，提取结构化数据(生成item)</code></li><li><code>2.生成需要下一页的请求URL。</code></li></ol><p>以下为我们的第一个Spider代码，保存在 tutorial&#x2F;spiders 目录下的 tencent_spider.py 文件中:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>f = open(&#39;tengxun.txt&#39;, &#39;wb&#39;)</code></p></li><li><p><code>f.write(response.body)</code></p></li><li><p><code>f.close()</code></p></li></ol><h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h3><p>进入项目的根目录，执行下列命令启动spider:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent</code></li></ol><p>crawl tencent 启动用于爬取 tencent 的spider，您将得到类似的输出:</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/919526005b0a52806c14376101fa97cd.gif" alt="入门教程 - 图2"></p><p>现在，查看当前目录，会注意到有文件被创建了: tengxun.txt,正如我们的 parse 方法里做的一样。</p><p><strong>注意，在刚启动的时候会有一段error信息，不用理会</strong></p><p>在第六天作业里面有说明原因</p><p> 复制代码</p><ol><li><code>2016-08-11 13:07:35 [boto] ERROR: Caught exception reading instance data</code></li><li><code>Traceback (most recent call last):</code></li><li><code>File &quot;/usr/lib/python2.7/dist-packages/boto/utils.py&quot;, line 210, in retry_url</code></li><li><code>r = opener.open(req, timeout=timeout)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 429, in open</code></li><li><code>response = self._open(req, data)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 447, in _open</code></li><li><code>&#39;_open&#39;, req)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 407, in _call_chain</code></li><li><code>result = func(*args)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1228, in http_open</code></li><li><code>return self.do_open(httplib.HTTPConnection, req)</code></li><li><code>File &quot;/usr/lib/python2.7/urllib2.py&quot;, line 1198, in do_open</code></li><li><code>raise URLError(err)</code></li><li><code>URLError: &lt;urlopen error timed out&gt;</code></li></ol><h3 id="刚才发生了什么？"><a href="#刚才发生了什么？" class="headerlink" title="刚才发生了什么？"></a>刚才发生了什么？</h3><p>Scrapy为Spider的 start_urls 属性中的每个URL创建了 <code>scrapy.Request</code> 对象，并将 parse 方法作为回调函数(callback)赋值给了Request。</p><p>Request对象经过调度，执行生成 <code>scrapy.http.Response</code> 对象并送回给<code>parse()</code> 方法。</p><h3 id="提取Item"><a href="#提取Item" class="headerlink" title="提取Item"></a>提取Item</h3><h4 id="Selectors选择器简介"><a href="#Selectors选择器简介" class="headerlink" title="Selectors选择器简介"></a>Selectors选择器简介</h4><p><code>Scrapy Selectors</code> 内置<code>XPath</code> 和 <code>CSS Selector</code> 表达式机制</p><p>XPath表达式的例子及对应的含义:</p><p> 复制代码</p><ol><li><code>/html/head/title: 选择&lt;HTML&gt;文档中 &lt;head&gt; 标签内的 &lt;title&gt; 元素</code></li><li><code>/html/head/title/text(): 选择上面提到的 &lt;title&gt; 元素的文字</code></li><li><code>//td: 选择所有的 &lt;td&gt; 元素</code></li><li><code>//div[@class=&quot;mine&quot;]: 选择所有具有 class=&quot;mine&quot; 属性的 div 元素</code></li></ol><p>Selector有四个基本的方法:</p><p> 复制代码</p><ol><li><code>xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。</code></li><li><code>css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.</code></li><li><code>extract(): 序列化该节点为unicode字符串并返回list。</code></li><li><code>re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。</code></li></ol><h3 id="尝试Selector选择器"><a href="#尝试Selector选择器" class="headerlink" title="尝试Selector选择器"></a>尝试Selector选择器</h3><p>为了介绍Selector的使用方法，接下来我们将要使用内置的 scrapy shell 。Scrapy Shell需要您预装好IPython(一个扩展的Python终端)。</p><p>您需要进入项目的根目录，执行下列命令来启动shell:</p><p> 复制代码</p><ol><li><code>scrapy shell &quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></li></ol><p>注解:当您在终端运行Scrapy时，请一定记得给url地址加上引号，否则包含参数的url(例如 &amp; 字符)会导致Scrapy运行失败。</p><p>shell的输出类似:</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/2a926bfd0a51273e640617660ec14b76.gif" alt="入门教程 - 图3"></p><p>当shell载入后，将得到一个包含response数据的本地 <code>response</code> 变量。输入 <code>response.body</code>将输出response的包体， 输出 <code>response.headers</code> 可以看到response的包头。</p><ul><li>当输入 <code>response.selector</code> 时， 将获取到一个response 初始化的类 <code>Selector</code> 的对象</li><li>此时，可以通过使用 response.selector.xpath() 或 response.selector.css() 来对 response 进行查询。</li><li>或者，scrapy也对 response.selector.xpath() 及 response.selector.css() 提供了一些快捷方式, 例如 response.xpath() 或 response.css()</li></ul><p>让我们来试试:</p><p> 复制代码</p><ol><li><p><code>response.xpath(&#39;//title&#39;)</code></p></li><li><p><code>[&lt;Selector xpath=&#39;//title&#39; data=u&#39;&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title&#39;&gt;]</code></p></li><li><p><code>response.xpath(&#39;//title&#39;).extract()</code></p></li><li><p><code>[u&#39;&lt;title&gt;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&lt;/title&gt;&#39;]</code></p></li><li><p><code>print response.xpath(&#39;//title&#39;).extract()[0]</code></p></li><li><p><code>&lt;title&gt;职位搜索 | 社会招聘 | Tencent 腾讯招聘&lt;/title&gt;</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;)</code></p></li><li><p><code>&lt;Selector xpath=&#39;//title/text()&#39; data=u&#39;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&#39;&gt;</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;)[0].extract()</code></p></li><li><p><code>u&#39;\u804c\u4f4d\u641c\u7d22 | \u793e\u4f1a\u62db\u8058 | Tencent \u817e\u8baf\u62db\u8058&#39;</code></p></li><li><p><code>print response.xpath(&#39;//title/text()&#39;)[0].extract()</code></p></li><li><p><code>职位搜索 | 社会招聘 | Tencent 腾讯招聘</code></p></li><li><p><code>response.xpath(&#39;//title/text()&#39;).re(&#39;(\w+):&#39;)</code></p></li><li><p><code>[u&#39;\u804c\u4f4d\u641c\u7d22&#39;,</code></p></li><li><p><code>u&#39;\u793e\u4f1a\u62db\u8058&#39;,</code></p></li><li><p><code>u&#39;Tencent&#39;,</code></p></li><li><p><code>u&#39;\u817e\u8baf\u62db\u8058&#39;]</code></p></li></ol><h3 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h3><p>现在，我们来尝试从这些页面中提取些有用的数据。</p><p>我们可以通过XPath选择该页面中网站列表里所有 <code>lass=even</code> 元素:</p><p> 复制代码</p><ol><li><code>site = response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;)</code></li></ol><p>职位名称:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></li><li><code>TEG15-运营开发工程师（深圳）</code></li></ol><p>职位名称详情页:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></li><li><code>position_detail.php?id=20744&amp;keywords=&amp;tid=0&amp;lid=0</code></li></ol><p>职位类别:</p><p> 复制代码</p><ol><li><code>print site[0].xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></li><li><code>技术类</code></li></ol><p>对于 <code>.xpath()</code> 调用返回<code>selector</code>组成的<code>list</code>，因此可以拼接更多的 .xpath() 来进一步获取某个节点。</p><p> 复制代码</p><ol><li><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></li><li><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></li><li><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></li><li><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></li><li><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></li><li><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></li><li><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></li><li><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></li></ol><p>在我们的<code>tencent_spider.py</code>文件修改成如下代码:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></p></li><li><p><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></p></li><li><p><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></p></li><li><p><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></p></li><li><p><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></p></li><li><p><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></p></li><li><p><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></p></li><li><p><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></p></li></ol><p>如图所示：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/10efeb366ce0b9a924778f8269880ba5.png" alt="入门教程 - 图4"></p><p>现在尝试再次爬取<code>hr.tencent.com</code>，您将看到爬取到的网站信息被成功输出:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent</code></li></ol><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/f8605ad6da5b180c4ba5bd0f82c0e16d.gif" alt="入门教程 - 图5"></p><h3 id="使用item"><a href="#使用item" class="headerlink" title="使用item"></a>使用item</h3><p>Item 对象是自定义的python字典。可以使用标准的字典语法来获取到其每个字段的值。</p><p>输入 `scrapy shell’</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>class RecruitItem(scrapy.Item):</code></p></li><li><p><code>name = scrapy.Field()</code></p></li><li><p><code>detailLink = scrapy.Field()</code></p></li><li><p><code>catalog = scrapy.Field()</code></p></li><li><p><code>recruitNumber = scrapy.Field()</code></p></li><li><p><code>workLocation = scrapy.Field()</code></p></li><li><p><code>publishTime = scrapy.Field()</code></p></li><li><p><code>item = RecruitItem()</code></p></li><li><p><code>item[&#39;name&#39;] = &#39;sanlang&#39;</code></p></li><li><p><code>item[&#39;name&#39;]</code></p></li><li><p><code>&#39;sanlang&#39;</code></p></li></ol><p>一般来说，Spider将会将爬取到的数据以Item对象返回。所以为了将爬取的数据返回，最终<code>tencent_spider.py</code>代码将是:</p><p> 复制代码</p><ol><li><p><code>import scrapy</code></p></li><li><p><code>from tutorial.items import RecruitItem</code></p></li><li><p><code>class RecruitSpider(scrapy.spiders.Spider):</code></p></li><li><p><code>name = &quot;tencent&quot;</code></p></li><li><p><code>allowed_domains = [&quot;hr.tencent.com&quot;]</code></p></li><li><p><code>start_urls = [</code></p></li><li><p><code>&quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</code></p></li><li><p><code>]</code></p></li><li><p><code>def parse(self, response):</code></p></li><li><p><code>for sel in response.xpath(&#39;//*[@class=&quot;even&quot;]&#39;):</code></p></li><li><p><code>name = sel.xpath(&#39;./td[1]/a/text()&#39;).extract()[0]</code></p></li><li><p><code>detailLink = sel.xpath(&#39;./td[1]/a/@href&#39;).extract()[0]</code></p></li><li><p><code>catalog = sel.xpath(&#39;./td[2]/text()&#39;).extract()[0]</code></p></li><li><p><code>recruitNumber = sel.xpath(&#39;./td[3]/text()&#39;).extract()[0]</code></p></li><li><p><code>workLocation = sel.xpath(&#39;./td[4]/text()&#39;).extract()[0]</code></p></li><li><p><code>publishTime = sel.xpath(&#39;./td[5]/text()&#39;).extract()[0]</code></p></li><li><p><code>print name, detailLink, catalog,recruitNumber,workLocation,publishTime</code></p></li><li><p><code>item = RecruitItem()</code></p></li><li><p><code>item[&#39;name&#39;]=name.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;detailLink&#39;]=detailLink.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;catalog&#39;]=catalog.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;recruitNumber&#39;]=recruitNumber.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;workLocation&#39;]=workLocation.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>item[&#39;publishTime&#39;]=publishTime.encode(&#39;utf-8&#39;)</code></p></li><li><p><code>yield item</code></p></li></ol><p>现在对<code>hr.tencent.com</code>进行爬取将会产生 RecruitItem 对象:</p><p>运行过程：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/0dc4e83404d6b04c66252fcff2123a97.gif" alt="入门教程 - 图6"></p><h3 id="保存爬取到的数据"><a href="#保存爬取到的数据" class="headerlink" title="保存爬取到的数据"></a>保存爬取到的数据</h3><p>最简单存储爬取的数据的方式是使用 <code>Feed exports</code>:</p><p> 复制代码</p><ol><li><code>scrapy crawl tencent -o items.json</code></li></ol><p>该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。</p><p>如果需要对爬取到的item做更多更为复杂的操作，您可以编写 Item Pipeline 。 类似于我们在创建项目时对Item做的，用于您编写自己的 tutorial&#x2F;pipelines.py 也被创建。 不过如果您仅仅想要保存item，您不需要实现任何的pipeline。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;入门教程&quot;&gt;&lt;a href=&quot;#入门教程&quot; class=&quot;headerlink&quot; title=&quot;入门教程&quot;&gt;&lt;/a&gt;入门教程&lt;/h1&gt;&lt;h4 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Regular Expression</title>
    <link href="http://example.com/2022/05/06/Regular-Expression/"/>
    <id>http://example.com/2022/05/06/Regular-Expression/</id>
    <published>2022-05-06T12:55:53.000Z</published>
    <updated>2022-05-06T13:20:16.139Z</updated>
    
    <content type="html"><![CDATA[<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p><strong>掌握了XPath、CSS选择器，为什么还要学习正则？</strong></p><p>正则表达式，用标准正则解析，一般会把HTML当做普通文本，用指定格式匹配当相关文本，适合小片段文本，或者某一串字符(比如电话号码、邮箱账户)，或者HTML包含javascript的代码，无法用CSS选择器或者XPath</p><p><a href="http://tool.oschina.net/regex/">在线正则表达式测试网站</a></p><p><a href="https://docs.python.org/2/library/re.html#regular-expression-objects">官方文档</a></p><p><strong>了解正则表达式</strong></p><p>正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个”规则字符串”，这个”规则字符串”用来表达对字符串的一种过滤逻辑。</p><h3 id="正则表达式常见概念"><a href="#正则表达式常见概念" class="headerlink" title="正则表达式常见概念"></a>正则表达式常见概念</h3><ul><li><p>边界匹配</p><p>  ^ — 与字符串开始的地方匹配，不匹配任何字符；</p><p>  $ — 与字符串结束的地方匹配，不匹配任何字符；</p></li></ul><p> 复制代码</p><ol><li><code>str = &quot;cat abdcatdetf ios&quot;</code></li><li><code>^cat : 验证该行以c开头紧接着是a，然后是t</code></li><li><code>ios$ : 验证该行以t结尾倒数第二个字符为a倒数第三个字符为c</code></li><li><code>^cat$: 以c开头接着是a-&gt;t然后是行结束：只有cat三个字母的数据行</code></li><li><code>^$ : 开头之后马上结束：空白行，不包括任何字符</code></li><li><code>^ : 行的开头，可以匹配任何行，因为每个行都有行开头</code></li></ol><p>\b — 匹配一个单词边界，也就是单词和空格之间的位置，不匹配任何字符；</p><p> 复制代码</p><ol><li><code>&quot;er\b&quot;可以匹配&quot;never&quot;中的&quot;er&quot;，但不能匹配&quot;verb&quot;中的&quot;er&quot;。</code></li></ol><p>\B — \b取非，即匹配一个非单词边界；</p><p> 复制代码</p><ol><li><code>&quot;er\B&quot;能匹配&quot;verb&quot;中的&quot;er&quot;，但不能匹配&quot;never&quot;中的&quot;er&quot;。</code></li></ol><ul><li><p>数量词的贪婪模式与非贪婪模式</p><p>  正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：</p></li></ul><p> 复制代码</p><ol><li><code>正则表达式&quot;ab*&quot;如果用于查找&quot;abbbc&quot;，将找到&quot;abbb&quot;。而如果使用非贪婪的数量词&quot;ab*?&quot;，将找到&quot;a&quot;。</code></li></ol><ul><li>反斜杠问题</li></ul><p>与大多数编程语言相同，正则表达式里使用”\“作为转义字符，这就可能造成反斜杠困扰。</p><p>假如你需要匹配文本中的字符”\“，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\“：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。</p><p>Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\“表示。</p><p>同样，匹配一个数字的”\d”可以写成r”\d”。有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。</p><p> 复制代码</p><ol><li><code>import re</code></li><li><code>a=re.search(r&quot;\\&quot;,&quot;ab123bb\c&quot;)</code></li><li><code>print a.group()</code></li><li><code>\</code></li><li><code>a=re.search(r&quot;\d&quot;,&quot;ab123bb\c&quot;)</code></li><li><code>print a.group()</code></li><li><code>1</code></li></ol><h3 id="Python-Re模块"><a href="#Python-Re模块" class="headerlink" title="Python Re模块"></a>Python Re模块</h3><p>Python 自带了re模块，它提供了对正则表达式的支持。</p><h3 id="match函数"><a href="#match函数" class="headerlink" title="match函数"></a>match函数</h3><p>re.match 尝试从字符串的<strong>起始位置</strong>匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。</p><p>下面是此函数的语法：</p><p> 复制代码</p><ol><li><code>re.match(pattern, string, flags=0)</code></li></ol><p>这里的参数的说明：</p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>pattern</td><td>这是正则表达式来进行匹配。</td></tr><tr><td>string</td><td>这是字符串，这将被搜索匹配的模式，在字符串的开头。</td></tr><tr><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</td></tr></tbody></table><p>匹配成功re.match方法返回一个匹配的对象，否则返回None。</p><p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p><table><thead><tr><th>匹配对象的方法</th><th>描述</th></tr></thead><tbody><tr><td>group(num&#x3D;0)</td><td>此方法返回整个匹配（或指定分组num）</td></tr><tr><td>groups()</td><td>此方法返回所有元组匹配的子组（空，如果没有）</td></tr></tbody></table><h3 id="例子："><a href="#例子：" class="headerlink" title="例子："></a>例子：</h3><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;</code></p></li><li><p><code>matchObj = re.match( r&#39;(.*) are (.*?) .*&#39;, line, re.M|re.I)</code></p></li><li><p><code>if matchObj:</code></p></li><li><p><code>print &quot;matchObj.group() : &quot;, matchObj.group()</code></p></li><li><p><code>print &quot;matchObj.group(1) : &quot;, matchObj.group(1)</code></p></li><li><p><code>print &quot;matchObj.group(2) : &quot;, matchObj.group(2)</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;No match!!&quot;</code></p></li></ol><p>当执行上面的代码，它产生以下结果：</p><p> 复制代码</p><ol><li><code>matchObj.group() : Cats are smarter than dogs</code></li><li><code>matchObj.group(1) : Cats</code></li><li><code>matchObj.group(2) : smarter</code></li></ol><h4 id="正则表达式修饰符-选项标志"><a href="#正则表达式修饰符-选项标志" class="headerlink" title="正则表达式修饰符 - 选项标志"></a>正则表达式修饰符 - 选项标志</h4><p>正则表达式字面可以包含一个可选的修饰符来控制匹配的各个方面。修饰符被指定为一个可选的标志。可以使用异或提供多个修饰符（|），如先前所示，并且可以由这些中的一个来表示：</p><table><thead><tr><th>修饰符</th><th>描述</th></tr></thead><tbody><tr><td>re.I(re.IGNORECASE)</td><td>使匹配对大小写不敏感</td></tr><tr><td>re.M(MULTILINE)</td><td>多行匹配，影响 ^ 和 $</td></tr><tr><td>re.S(DOTALL)</td><td>使 . 匹配包括换行在内的所有字符</td></tr><tr><td>re.X(VERBOSE)</td><td>正则表达式可以是多行，忽略空白字符，并可以加入注释</td></tr></tbody></table><h3 id="findall-函数"><a href="#findall-函数" class="headerlink" title="findall()函数"></a>findall()函数</h3><p>re.findall(pattern, string, flags&#x3D;0)</p><p>返回字符串中所有模式的非重叠的匹配，作为字符串列表。该字符串扫描左到右，并匹配返回的顺序发现</p><p> 复制代码</p><ol><li><p><code>默认：</code></p></li><li><p><code>pattren = &quot;\w+&quot;</code></p></li><li><p><code>target = &quot;hello world\nWORLD HELLO&quot;</code></p></li><li><p><code>re.findall(pattren,target)</code></p></li><li><p><code>[&#39;hello&#39;, &#39;world&#39;, &#39;WORLD&#39;, &#39;HELLO&#39;]</code></p></li><li><p><code>re.I:</code> </p></li><li><p><code>re.findall(&quot;world&quot;, target,re.I)</code></p></li><li><p><code>[&#39;world&#39;, &#39;WORLD&#39;]</code></p></li><li><p><code>re.S:</code> </p></li><li><p><code>re.findall(&quot;world.WORLD&quot;, target,re.S)</code></p></li><li><p><code>[&quot;world\nworld&quot;]</code></p></li><li><p><code>re.findall(&quot;hello.*WORLD&quot;, target,re.S)</code></p></li><li><p><code>[&#39;hello world\nWORLD&#39;]</code></p></li><li><p><code>re.M:</code></p></li><li><p><code>re.findall(&quot;^WORLD&quot;,target,re.M)</code></p></li><li><p><code>[&quot;WORLD&quot;]</code></p></li><li><p><code>re.X:</code></p></li><li><p><code>reStr = &#39;&#39;&#39;\d&#123;3&#125;  #区号</code></p></li><li><p><code>-\d&#123;8&#125;&#39;&#39;&#39; #号码</code></p></li><li><p><code>re.findall(reStr,&quot;010-12345678&quot;,re.X)</code> </p></li><li><p><code>[&quot;010-12345678&quot;]</code></p></li></ol><h3 id="search函数"><a href="#search函数" class="headerlink" title="search函数"></a>search函数</h3><p>re.search 扫描整个字符串并返回第一个成功的匹配。</p><p>下面是此函数语法：</p><p> 复制代码</p><ol><li><code>re.search(pattern, string, flags=0)</code></li></ol><p>这里的参数说明：</p><table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>pattern</td><td>这是正则表达式来进行匹配。</td></tr><tr><td>string</td><td>这是字符串，这将被搜索到的字符串中的任何位置匹配的模式。</td></tr><tr><td>flags</td><td>标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</td></tr></tbody></table><p>匹配成功re.search方法返回一个匹配的对象，否则返回None。</p><p>我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。</p><table><thead><tr><th>匹配对象的方法</th><th>描述</th></tr></thead><tbody><tr><td>group(num&#x3D;0)</td><td>此方法返回整个匹配（或指定分组num）</td></tr><tr><td>groups()</td><td>此方法返回所有元组匹配的子组（空，如果没有）</td></tr></tbody></table><h4 id="例子：-1"><a href="#例子：-1" class="headerlink" title="例子："></a>例子：</h4><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;;</code></p></li><li><p><code>searchObj = re.search( r&#39;(.*) are (.*?) .*&#39;, line, re.M|re.I)</code></p></li><li><p><code>if searchObj:</code></p></li><li><p><code>print &quot;searchObj.group() : &quot;, searchObj.group()</code></p></li><li><p><code>print &quot;searchObj.group(1) : &quot;, searchObj.group(1)</code></p></li><li><p><code>print &quot;searchObj.group(2) : &quot;, searchObj.group(2)</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;Nothing found!!&quot;</code></p></li></ol><p>当执行上面的代码，它产生以下结果：</p><p> 复制代码</p><ol><li><code>matchObj.group() : Cats are smarter than dogs</code></li><li><code>matchObj.group(1) : Cats</code></li><li><code>matchObj.group(2) : smarter</code></li></ol><h4 id="re-match与re-search的区别"><a href="#re-match与re-search的区别" class="headerlink" title="re.match与re.search的区别"></a>re.match与re.search的区别</h4><p>re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p><h4 id="例子：-2"><a href="#例子：-2" class="headerlink" title="例子："></a>例子：</h4><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>line = &quot;Cats are smarter than dogs&quot;;</code></p></li><li><p><code>matchObj = re.match( r&#39;dogs&#39;, line, re.M|re.I)</code></p></li><li><p><code>if matchObj:</code></p></li><li><p><code>print &quot;match --&gt; matchObj.group() : &quot;, matchObj.group()</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;No match!!&quot;</code></p></li><li><p><code>searchObj = re.search( r&#39;dogs&#39;, line, re.M|re.I)</code></p></li><li><p><code>if searchObj:</code></p></li><li><p><code>print &quot;search --&gt; searchObj.group() : &quot;, searchObj.group()</code></p></li><li><p><code>else:</code></p></li><li><p><code>print &quot;Nothing found!!&quot;</code></p></li></ol><p>当执行上面的代码，产生以下结果：</p><p> 复制代码</p><ol><li><code>No match!!</code></li><li><code>search --&gt; matchObj.group() : dogs</code></li></ol><h3 id="搜索和替换"><a href="#搜索和替换" class="headerlink" title="搜索和替换"></a>搜索和替换</h3><p>Python 的re模块提供了re.sub用于替换字符串中的匹配项。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p> 复制代码</p><ol><li><code>re.sub(pattern, repl, string, max=0)</code></li></ol><p>返回的字符串是在字符串中用 RE 最左边不重复的匹配来替换。如果模式没有发现，字符将被没有改变地返回。可选参数 count 是模式匹配后替换的最大次数；count 必须是非负整数。缺省值是 0 表示替换所有的匹配。实例：</p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>下面是一个爬虫做翻页面例子：</p><p> 复制代码</p><ol><li><p><code>#!/usr/bin/python</code></p></li><li><p><code>import re</code></p></li><li><p><code>url = &quot;http://hr.tencent.com/position.php?&amp;start=10&quot;</code></p></li><li><p><code>page = re.search(&#39;start=(\d+)&#39;,url).group(1)</code></p></li><li><p><code>nexturl = re.sub(r&#39;start=(\d+)&#39;, &#39;start=&#39;+str(int(page)+10), url)</code></p></li><li><p><code>print &quot;Next Url : &quot;, nexturl</code></p></li></ol><p>当执行上面的代码，产生以下结果：</p><p> 复制代码</p><ol><li><code>Next Url : http://hr.tencent.com/position.php?&amp;start=20</code></li></ol><h3 id="正则表达式语法"><a href="#正则表达式语法" class="headerlink" title="正则表达式语法"></a>正则表达式语法</h3><p>下表列出了Python中可用正则表达式语法：</p><p><img src="https://static.sitestack.cn/projects/piaosanlang-spiders/4fdd105dc5ac24d05a59d8f215937e45.png" alt="非结构化数据之正则表达式 - 图1"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;正则表达式&quot;&gt;&lt;a href=&quot;#正则表达式&quot; class=&quot;headerlink&quot; title=&quot;正则表达式&quot;&gt;&lt;/a&gt;正则表达式&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;掌握了XPath、CSS选择器，为什么还要学习正则？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;正则表达式，用</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/04/29/hello-world/"/>
    <id>http://example.com/2022/04/29/hello-world/</id>
    <published>2022-04-29T14:27:05.667Z</published>
    <updated>2022-05-04T11:20:17.501Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
